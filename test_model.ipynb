{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a28fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_nlg import RecipeNLGDataset, TokenizedRecipeNLGDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import kagglehub\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "path = kagglehub.dataset_download(\"paultimothymooney/recipenlg\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv(path + \"/RecipeNLG_dataset.csv\", header=0)\n",
    "# Create an instance of the RecipeNLGDataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093b7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_path = Path(\"title_to_all_tokenizer\")\n",
    "print(\"Loading tokenizer\")\n",
    "hf_tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path, model_max_lenth=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212c36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'all' is default mode\n",
    "data = RecipeNLGDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3c7e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1146,    12,   300,  ...,     0,     0,     0],\n",
       "        [13024,    66,   842,  ...,     0,     0,     0],\n",
       "        [ 1425,   442,     2,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1767,   610,   679,  ...,     0,     0,     0],\n",
       "        [ 1725,  1183,     9,  ...,     0,     0,     0],\n",
       "        [  327,  1604,   348,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_batch = data.recipe_strings[:16]\n",
    "\n",
    "tokenized_batch = hf_tokenizer(\n",
    "    text=recipe_batch.tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokenized_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4a9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proper_format = {\n",
    "    \"input_ids\": tokenized_batch[\"input_ids\"].squeeze(0),\n",
    "    \"attention_mask\": tokenized_batch[\"attention_mask\"].squeeze(0),\n",
    "    # etc.\n",
    "}\n",
    "\n",
    "print(proper_format[\"attention_mask\"].shape)\n",
    "proper_format[\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016dd65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 16\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "hf_ds = Dataset.from_dict({\n",
    "    k: v.numpy()  # Datasets accepts numpy arrays\n",
    "    for k, v in proper_format.items()\n",
    "})\n",
    "\n",
    "print(hf_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f245269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    # Convert each field in the batch to a PyTorch tensor\n",
    "    return {\n",
    "        key: torch.stack([torch.tensor(item[key]) for item in batch])\n",
    "        for key in batch[0]\n",
    "    }\n",
    "\n",
    "loader = DataLoader(hf_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    print(batch['input_ids'].shape) # (batch size, max_length) -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb3ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicken funny <end_title> 1 large whole chicken , 2 ( 10 12 oz . ) cans chicken gravy , 1 ( 10 12 oz . ) can cream of mushroom soup , 1 ( 6 oz . ) box stove top stuffing , 4 oz . shredded cheese <end_ingredients> boil and debone chicken . put bite size pieces in average size square casserole dish . pour gravy and cream of mushroom soup over chicken ; level . make stuffing according to instructions on box ( do not make too moist ) . put stuffing on top of chicken and gravy ; level . sprinkle shredded cheese on top and bake at 350u00b0 for approximately 20 minutes or until golden and bubbly . <end> [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure decoding works\n",
    "first_batch = next(iter(loader))\n",
    "first_example = first_batch['input_ids'][0]\n",
    "hf_tokenizer.decode(first_example)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dcddc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 66])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_ids = first_batch['input_ids']\n",
    "\n",
    "vocab_size = len(hf_tokenizer.get_vocab())\n",
    "d_model = 66\n",
    "context_length = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# embedding layer: one row for every token in vocab, embedding-size columns\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "\n",
    "x = embedding(input_ids)\n",
    "x.shape # (batchsize x seq_length x embed dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef8ee89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPositionalEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositionalEncoder\n\u001b[0;32m----> 5\u001b[0m pe \u001b[38;5;241m=\u001b[39m PositionalEncoder(d_model\u001b[38;5;241m=\u001b[39md_model, context_len\u001b[38;5;241m=\u001b[39mcontext_length)\n\u001b[1;32m      7\u001b[0m pe\u001b[38;5;241m.\u001b[39mpe\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd_model' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from PositionalEncoder import PositionalEncoder\n",
    "\n",
    "\n",
    "pe = PositionalEncoder(d_model=d_model, context_len=context_length)\n",
    "\n",
    "pe.pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8ecf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 66])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it gets through the positional encoder!\n",
    "x = pe(x)\n",
    "\n",
    "x.shape # shape (batch, context, dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0837fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'FFN.FFN(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoderBlock\n\u001b[0;32m----> 3\u001b[0m test_decoder \u001b[38;5;241m=\u001b[39m DecoderBlock(num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, d_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NLP/NextByte/models.py:21\u001b[0m, in \u001b[0;36mDecoderBlock.__init__\u001b[0;34m(self, d_model, num_heads, d_hidden, num_hidden_layers)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# layer norm applies to each embedding vector individual, so it needs the size of those vectors (d_model)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(d_model)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;241m=\u001b[39m FFN(d_input\u001b[38;5;241m=\u001b[39md_model, d_output\u001b[38;5;241m=\u001b[39md_model, d_hidden\u001b[38;5;241m=\u001b[39md_hidden, num_hidden_layers\u001b[38;5;241m=\u001b[39mnum_hidden_layers)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'FFN.FFN(...)'?"
     ]
    }
   ],
   "source": [
    "from models import DecoderBlock\n",
    "\n",
    "test_decoder = DecoderBlock(num_heads=2, d_model=d_model, num_hidden_layers=2, d_hidden=2048)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
