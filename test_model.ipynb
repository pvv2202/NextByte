{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a28fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_nlg import RecipeNLGDataset, TokenizedRecipeNLGDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import kagglehub\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "path = kagglehub.dataset_download(\"paultimothymooney/recipenlg\")\n",
    "# Load the dataset\n",
    "df = pd.read_csv(path + \"/RecipeNLG_dataset.csv\", header=0)\n",
    "# Create an instance of the RecipeNLGDataset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093b7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_path = Path(\"title_to_all_tokenizer\")\n",
    "print(\"Loading tokenizer\")\n",
    "hf_tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path, model_max_lenth=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'all' is default mode\n",
    "data = RecipeNLGDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d3c7e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1146,    12,   300,  ...,     0,     0,     0],\n",
       "        [13024,    66,   842,  ...,     0,     0,     0],\n",
       "        [ 1425,   442,     2,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1767,   610,   679,  ...,     0,     0,     0],\n",
       "        [ 1725,  1183,     9,  ...,     0,     0,     0],\n",
       "        [  327,  1604,   348,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_batch = data.recipe_strings[:16]\n",
    "\n",
    "tokenized_batch = hf_tokenizer(\n",
    "    text=recipe_batch.tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "tokenized_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c4a9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proper_format = {\n",
    "    \"input_ids\": tokenized_batch[\"input_ids\"].squeeze(0),\n",
    "    \"attention_mask\": tokenized_batch[\"attention_mask\"].squeeze(0),\n",
    "    # etc.\n",
    "}\n",
    "\n",
    "print(proper_format[\"attention_mask\"].shape)\n",
    "proper_format[\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "016dd65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 16\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "hf_ds = Dataset.from_dict({\n",
    "    k: v.numpy()  # Datasets accepts numpy arrays\n",
    "    for k, v in proper_format.items()\n",
    "})\n",
    "\n",
    "print(hf_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f245269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    # Convert each field in the batch to a PyTorch tensor\n",
    "    return {\n",
    "        key: torch.stack([torch.tensor(item[key]) for item in batch])\n",
    "        for key in batch[0]\n",
    "    }\n",
    "\n",
    "loader = DataLoader(hf_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    print(batch['input_ids'].shape) # (batch size, max_length) -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rhubarb coffee cake <end_title> 1 12 c . sugar , 12 c . butter , 1 egg , 1 c . buttermilk , 2 c . flour , 12 tsp . salt , 1 tsp . soda , 1 c . buttermilk , 2 c . rhubarb , finely cut , 1 tsp . vanilla <end_ingredients> cream sugar and butter . add egg and beat well . to creamed butter , sugar and egg , add alternately buttermilk with mixture of flour , salt and soda . mix well . add rhubarb and vanilla . pour into greased 9 x 13 - inch pan and add topping . <end> [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure decoding works\n",
    "first_batch = next(iter(loader))\n",
    "first_example = first_batch['input_ids'][0]\n",
    "hf_tokenizer.decode(first_example)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dcddc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 65])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_ids = first_batch['input_ids']\n",
    "\n",
    "vocab_size = len(hf_tokenizer.get_vocab())\n",
    "d_model = 65\n",
    "context_length = len(input_ids)\n",
    "\n",
    "\n",
    "\n",
    "# embedding layer: one row for every token in vocab, embedding-size columns\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "\n",
    "x = embedding(input_ids)\n",
    "x.shape # (batchsize x seq_length x embed dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ef8ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PositionalEncoder import PositionalEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f09e046d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'arrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pe \u001b[38;5;241m=\u001b[39m PositionalEncoder(context_len\u001b[38;5;241m=\u001b[39mcontext_length, d_model\u001b[38;5;241m=\u001b[39md_model)\n",
      "File \u001b[0;32m~/Desktop/NLP/NextByte/PositionalEncoder.py:24\u001b[0m, in \u001b[0;36mPositionalEncoder.__init__\u001b[0;34m(self, context_len, d_model, pdrop)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39mpdrop)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# create a tensor holding every possible position in the input sequence\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marrange(\u001b[38;5;241m0\u001b[39m, context_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# shape: (context_len x 1)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# by a bunch of annoying log properties this expression is equal to the dividing term\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# in the paper, and allows for more numerical stability, removing the need to calculate\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# exponents with 10000 as the base\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 2i just means all the even numbers in d_model the torch.arrange below accomplishes that\u001b[39;00m\n\u001b[1;32m     30\u001b[0m div_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39marrange(\u001b[38;5;241m0\u001b[39m, d_model, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m d_model) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10000.0\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/__init__.py:2681\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2681\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'arrange'"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoder(context_len=context_length, d_model=d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
